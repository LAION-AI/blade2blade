trainer:
  _target_: transformers.TrainingArguments
  output_dir: .
  learning_rate: 1.0e-05
  gradient_checkpointing: false
  gradient_accumulation_steps: 2
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1.0e-12
  weight_decay: 0.0
  warmup_steps: 10
  eval_steps: 50
  save_steps: 100
  num_train_epochs: 2
  logging_steps: 10
  max_grad_norm: 2.0
  save_total_limit: 4
  fp16: false
  log_wandb: false
padding_side: right
truncation_side: right
model: t5-small
epochs: 1
batch_size: 8
log_dir: safetyfiles
log_wandb: false
wandb_entity: shahules786
max_length: 512
per_digit_tokens: false
special_tokens:
  eos_token: </s>
  sep_token: <sep>
  pad_token: <pad>
dataset:
  name: allenai/prosocial-dialog
  train:
  - train
  - validation
  validation:
  - test
